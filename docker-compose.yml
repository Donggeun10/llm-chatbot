version: "3.8"
name: chatbot-ollama-rest
services:
    llm:
        image: ollama-gpu:local
        build:
            context: ./
            dockerfile: Dockerfile_ollama
        container_name: 'ollama'
        hostname: 'ollama'
        ports:
            - 11435:11434
        volumes:
            - ../ollama-model/models:/home/appuser/.ollama/models
        restart: 'always'
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities: [gpu]
    webapp:
        image: demo-chatbot:local
        build:
            context: ./
            dockerfile: Dockerfile
        ports:
            - "9090:8080"
        environment:
            - SPRING_PROFILES_ACTIVE=exaone
            - APPLICATION_PDF=pdf
        deploy:
            resources:
                reservations:
                    cpus: 2
                    memory: 2048m