name: chatbot-ollama-rest
services:
    llm:
        image: ollama-gpu:local
        build:
            context: ./
            dockerfile: Dockerfile_ollama
        container_name: 'ollama'
        hostname: 'ollama'
        ports:
            - 11435:11434
        volumes:
            - ../ollama-model/models:/home/appuser/.ollama/models
            - ../ollama-model/safetensors:/home/appuser/safetensors
        restart: 'always'
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities: [gpu]
                    cpus: 8
                    memory: 16G
    webapp:
        image: demo-chatbot:local
        build:
            context: ./
            dockerfile: Dockerfile
        ports:
            - "9090:8080"
        environment:
            - SPRING_PROFILES_ACTIVE=exaone
            - SPRING_DATA_REDIS_PORT=7379
            - APPLICATION_PDF=pdf
        deploy:
            resources:
                reservations:
                    cpus: 2
                    memory: 2048m
    redis:
        image: redis:local
        build:
            context: ./
            dockerfile: Dockerfile_redis
        ports:
            - "7379:6379"
        restart: 'always'
        deploy:
            resources:
                reservations:
                    cpus: 1
                    memory: 1024m
    gguf:
        image: llama_cpp:local
        build:
            context: ./
            dockerfile: Dockerfile_llama_cpp
        container_name: 'llama_cpp'
        hostname: 'llama_cpp'
        volumes:
            - ../ollama-model/safetensors:/home/appuser/safetensors
        deploy:
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          count: all
                          capabilities: [gpu]